2025-12-13 20:48:30,720 INFO Made checkpoint directory: "./hotel_best_model_aesc\2025-12-13-20-48-30"
2025-12-13 20:48:30,720 INFO ============ Initialed with 1 GPU(s) =============
2025-12-13 20:48:30,720 INFO dataset: [['hotel_review', './src/data/jsons/hotel_info.json']]
2025-12-13 20:48:30,720 INFO checkpoint_dir: ./hotel_best_model_aesc
2025-12-13 20:48:30,720 INFO bart_model: facebook/bart-base
2025-12-13 20:48:30,720 INFO log_dir: hotel_aesc
2025-12-13 20:48:30,720 INFO model_config: config/pretrain_base.json
2025-12-13 20:48:30,720 INFO text_only: False
2025-12-13 20:48:30,720 INFO checkpoint: ./checkpoint/pytorch_model.bin
2025-12-13 20:48:30,720 INFO lr_decay_every: 4
2025-12-13 20:48:30,720 INFO lr_decay_ratio: 0.8
2025-12-13 20:48:30,720 INFO epochs: 20
2025-12-13 20:48:30,736 INFO eval_every: 1
2025-12-13 20:48:30,736 INFO lr: 4e-05
2025-12-13 20:48:30,736 INFO num_beams: 4
2025-12-13 20:48:30,736 INFO continue_training: False
2025-12-13 20:48:30,736 INFO warmup: 0.1
2025-12-13 20:48:30,736 INFO dropout: None
2025-12-13 20:48:30,736 INFO classif_dropout: None
2025-12-13 20:48:30,736 INFO attention_dropout: None
2025-12-13 20:48:30,736 INFO activation_dropout: None
2025-12-13 20:48:30,736 INFO grad_clip: 5.0
2025-12-13 20:48:30,736 INFO gpu_num: 1
2025-12-13 20:48:30,736 INFO cpu: False
2025-12-13 20:48:30,736 INFO amp: False
2025-12-13 20:48:30,736 INFO master_port: 12355
2025-12-13 20:48:30,736 INFO batch_size: 16
2025-12-13 20:48:30,736 INFO seed: 2026
2025-12-13 20:48:30,736 INFO num_workers: 0
2025-12-13 20:48:30,736 INFO max_len: 10
2025-12-13 20:48:30,736 INFO max_len_a: 0.6
2025-12-13 20:48:30,736 INFO bart_init: 1
2025-12-13 20:48:30,736 INFO check_info: 
2025-12-13 20:48:30,736 INFO is_check: 1
2025-12-13 20:48:30,736 INFO task: 
2025-12-13 20:48:30,736 INFO Loading model...
2025-12-13 20:48:30,736 DEBUG Starting new HTTPS connection (1): huggingface.co:443
2025-12-13 20:48:31,966 DEBUG https://huggingface.co:443 "HEAD /facebook/bart-base/resolve/main/tokenizer_config.json HTTP/1.1" 404 0
2025-12-13 20:48:32,304 DEBUG https://huggingface.co:443 "GET /api/models/facebook/bart-base/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2025-12-13 20:48:32,608 DEBUG https://huggingface.co:443 "HEAD /facebook/bart-base/resolve/main/vocab.json HTTP/1.1" 307 0
2025-12-13 20:48:32,835 DEBUG https://huggingface.co:443 "HEAD /api/resolve-cache/models/facebook/bart-base/aadd2ab0ae0c8268c7c9693540e9904811f36177/vocab.json HTTP/1.1" 200 0
2025-12-13 20:48:32,971 INFO loading weights file ./checkpoint/pytorch_model.bin
2025-12-13 20:48:33,298 DEBUG https://huggingface.co:443 "HEAD /facebook/bart-base/resolve/main/config.json HTTP/1.1" 307 0
2025-12-13 20:48:33,514 DEBUG https://huggingface.co:443 "HEAD /api/resolve-cache/models/facebook/bart-base/aadd2ab0ae0c8268c7c9693540e9904811f36177/config.json HTTP/1.1" 200 0
2025-12-13 20:48:33,827 DEBUG https://huggingface.co:443 "HEAD /facebook/bart-base/resolve/main/config.json HTTP/1.1" 307 0
2025-12-13 20:48:34,035 DEBUG https://huggingface.co:443 "HEAD /api/resolve-cache/models/facebook/bart-base/aadd2ab0ae0c8268c7c9693540e9904811f36177/config.json HTTP/1.1" 200 0
2025-12-13 20:48:36,603 DEBUG https://huggingface.co:443 "HEAD /facebook/bart-base/resolve/main/tokenizer_config.json HTTP/1.1" 404 0
2025-12-13 20:48:36,937 DEBUG https://huggingface.co:443 "GET /api/models/facebook/bart-base/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2025-12-13 20:48:37,253 DEBUG https://huggingface.co:443 "HEAD /facebook/bart-base/resolve/main/vocab.json HTTP/1.1" 307 0
2025-12-13 20:48:37,467 DEBUG https://huggingface.co:443 "HEAD /api/resolve-cache/models/facebook/bart-base/aadd2ab0ae0c8268c7c9693540e9904811f36177/vocab.json HTTP/1.1" 200 0
2025-12-13 20:48:38,233 WARNING Some weights of the model checkpoint at ./checkpoint/pytorch_model.bin were not used when initializing MultiModalBartModelForPretrain: ['anp_decoder.decoder.embed_tokens.weight', 'anp_decoder.decoder.embed_positions.weight', 'anp_decoder.decoder.layers.0.self_attn.k_proj.weight', 'anp_decoder.decoder.layers.0.self_attn.k_proj.bias', 'anp_decoder.decoder.layers.0.self_attn.v_proj.weight', 'anp_decoder.decoder.layers.0.self_attn.v_proj.bias', 'anp_decoder.decoder.layers.0.self_attn.q_proj.weight', 'anp_decoder.decoder.layers.0.self_attn.q_proj.bias', 'anp_decoder.decoder.layers.0.self_attn.out_proj.weight', 'anp_decoder.decoder.layers.0.self_attn.out_proj.bias', 'anp_decoder.decoder.layers.0.self_attn_layer_norm.weight', 'anp_decoder.decoder.layers.0.self_attn_layer_norm.bias', 'anp_decoder.decoder.layers.0.encoder_attn.k_proj.weight', 'anp_decoder.decoder.layers.0.encoder_attn.k_proj.bias', 'anp_decoder.decoder.layers.0.encoder_attn.v_proj.weight', 'anp_decoder.decoder.layers.0.encoder_attn.v_proj.bias', 'anp_decoder.decoder.layers.0.encoder_attn.q_proj.weight', 'anp_decoder.decoder.layers.0.encoder_attn.q_proj.bias', 'anp_decoder.decoder.layers.0.encoder_attn.out_proj.weight', 'anp_decoder.decoder.layers.0.encoder_attn.out_proj.bias', 'anp_decoder.decoder.layers.0.encoder_attn_layer_norm.weight', 'anp_decoder.decoder.layers.0.encoder_attn_layer_norm.bias', 'anp_decoder.decoder.layers.0.fc1.weight', 'anp_decoder.decoder.layers.0.fc1.bias', 'anp_decoder.decoder.layers.0.fc2.weight', 'anp_decoder.decoder.layers.0.fc2.bias', 'anp_decoder.decoder.layers.0.final_layer_norm.weight', 'anp_decoder.decoder.layers.0.final_layer_norm.bias', 'anp_decoder.decoder.layers.1.self_attn.k_proj.weight', 'anp_decoder.decoder.layers.1.self_attn.k_proj.bias', 'anp_decoder.decoder.layers.1.self_attn.v_proj.weight', 'anp_decoder.decoder.layers.1.self_attn.v_proj.bias', 'anp_decoder.decoder.layers.1.self_attn.q_proj.weight', 'anp_decoder.decoder.layers.1.self_attn.q_proj.bias', 'anp_decoder.decoder.layers.1.self_attn.out_proj.weight', 'anp_decoder.decoder.layers.1.self_attn.out_proj.bias', 'anp_decoder.decoder.layers.1.self_attn_layer_norm.weight', 'anp_decoder.decoder.layers.1.self_attn_layer_norm.bias', 'anp_decoder.decoder.layers.1.encoder_attn.k_proj.weight', 'anp_decoder.decoder.layers.1.encoder_attn.k_proj.bias', 'anp_decoder.decoder.layers.1.encoder_attn.v_proj.weight', 'anp_decoder.decoder.layers.1.encoder_attn.v_proj.bias', 'anp_decoder.decoder.layers.1.encoder_attn.q_proj.weight', 'anp_decoder.decoder.layers.1.encoder_attn.q_proj.bias', 'anp_decoder.decoder.layers.1.encoder_attn.out_proj.weight', 'anp_decoder.decoder.layers.1.encoder_attn.out_proj.bias', 'anp_decoder.decoder.layers.1.encoder_attn_layer_norm.weight', 'anp_decoder.decoder.layers.1.encoder_attn_layer_norm.bias', 'anp_decoder.decoder.layers.1.fc1.weight', 'anp_decoder.decoder.layers.1.fc1.bias', 'anp_decoder.decoder.layers.1.fc2.weight', 'anp_decoder.decoder.layers.1.fc2.bias', 'anp_decoder.decoder.layers.1.final_layer_norm.weight', 'anp_decoder.decoder.layers.1.final_layer_norm.bias', 'anp_decoder.decoder.layers.2.self_attn.k_proj.weight', 'anp_decoder.decoder.layers.2.self_attn.k_proj.bias', 'anp_decoder.decoder.layers.2.self_attn.v_proj.weight', 'anp_decoder.decoder.layers.2.self_attn.v_proj.bias', 'anp_decoder.decoder.layers.2.self_attn.q_proj.weight', 'anp_decoder.decoder.layers.2.self_attn.q_proj.bias', 'anp_decoder.decoder.layers.2.self_attn.out_proj.weight', 'anp_decoder.decoder.layers.2.self_attn.out_proj.bias', 'anp_decoder.decoder.layers.2.self_attn_layer_norm.weight', 'anp_decoder.decoder.layers.2.self_attn_layer_norm.bias', 'anp_decoder.decoder.layers.2.encoder_attn.k_proj.weight', 'anp_decoder.decoder.layers.2.encoder_attn.k_proj.bias', 'anp_decoder.decoder.layers.2.encoder_attn.v_proj.weight', 'anp_decoder.decoder.layers.2.encoder_attn.v_proj.bias', 'anp_decoder.decoder.layers.2.encoder_attn.q_proj.weight', 'anp_decoder.decoder.layers.2.encoder_attn.q_proj.bias', 'anp_decoder.decoder.layers.2.encoder_attn.out_proj.weight', 'anp_decoder.decoder.layers.2.encoder_attn.out_proj.bias', 'anp_decoder.decoder.layers.2.encoder_attn_layer_norm.weight', 'anp_decoder.decoder.layers.2.encoder_attn_layer_norm.bias', 'anp_decoder.decoder.layers.2.fc1.weight', 'anp_decoder.decoder.layers.2.fc1.bias', 'anp_decoder.decoder.layers.2.fc2.weight', 'anp_decoder.decoder.layers.2.fc2.bias', 'anp_decoder.decoder.layers.2.final_layer_norm.weight', 'anp_decoder.decoder.layers.2.final_layer_norm.bias', 'anp_decoder.decoder.layers.3.self_attn.k_proj.weight', 'anp_decoder.decoder.layers.3.self_attn.k_proj.bias', 'anp_decoder.decoder.layers.3.self_attn.v_proj.weight', 'anp_decoder.decoder.layers.3.self_attn.v_proj.bias', 'anp_decoder.decoder.layers.3.self_attn.q_proj.weight', 'anp_decoder.decoder.layers.3.self_attn.q_proj.bias', 'anp_decoder.decoder.layers.3.self_attn.out_proj.weight', 'anp_decoder.decoder.layers.3.self_attn.out_proj.bias', 'anp_decoder.decoder.layers.3.self_attn_layer_norm.weight', 'anp_decoder.decoder.layers.3.self_attn_layer_norm.bias', 'anp_decoder.decoder.layers.3.encoder_attn.k_proj.weight', 'anp_decoder.decoder.layers.3.encoder_attn.k_proj.bias', 'anp_decoder.decoder.layers.3.encoder_attn.v_proj.weight', 'anp_decoder.decoder.layers.3.encoder_attn.v_proj.bias', 'anp_decoder.decoder.layers.3.encoder_attn.q_proj.weight', 'anp_decoder.decoder.layers.3.encoder_attn.q_proj.bias', 'anp_decoder.decoder.layers.3.encoder_attn.out_proj.weight', 'anp_decoder.decoder.layers.3.encoder_attn.out_proj.bias', 'anp_decoder.decoder.layers.3.encoder_attn_layer_norm.weight', 'anp_decoder.decoder.layers.3.encoder_attn_layer_norm.bias', 'anp_decoder.decoder.layers.3.fc1.weight', 'anp_decoder.decoder.layers.3.fc1.bias', 'anp_decoder.decoder.layers.3.fc2.weight', 'anp_decoder.decoder.layers.3.fc2.bias', 'anp_decoder.decoder.layers.3.final_layer_norm.weight', 'anp_decoder.decoder.layers.3.final_layer_norm.bias', 'anp_decoder.decoder.layers.4.self_attn.k_proj.weight', 'anp_decoder.decoder.layers.4.self_attn.k_proj.bias', 'anp_decoder.decoder.layers.4.self_attn.v_proj.weight', 'anp_decoder.decoder.layers.4.self_attn.v_proj.bias', 'anp_decoder.decoder.layers.4.self_attn.q_proj.weight', 'anp_decoder.decoder.layers.4.self_attn.q_proj.bias', 'anp_decoder.decoder.layers.4.self_attn.out_proj.weight', 'anp_decoder.decoder.layers.4.self_attn.out_proj.bias', 'anp_decoder.decoder.layers.4.self_attn_layer_norm.weight', 'anp_decoder.decoder.layers.4.self_attn_layer_norm.bias', 'anp_decoder.decoder.layers.4.encoder_attn.k_proj.weight', 'anp_decoder.decoder.layers.4.encoder_attn.k_proj.bias', 'anp_decoder.decoder.layers.4.encoder_attn.v_proj.weight', 'anp_decoder.decoder.layers.4.encoder_attn.v_proj.bias', 'anp_decoder.decoder.layers.4.encoder_attn.q_proj.weight', 'anp_decoder.decoder.layers.4.encoder_attn.q_proj.bias', 'anp_decoder.decoder.layers.4.encoder_attn.out_proj.weight', 'anp_decoder.decoder.layers.4.encoder_attn.out_proj.bias', 'anp_decoder.decoder.layers.4.encoder_attn_layer_norm.weight', 'anp_decoder.decoder.layers.4.encoder_attn_layer_norm.bias', 'anp_decoder.decoder.layers.4.fc1.weight', 'anp_decoder.decoder.layers.4.fc1.bias', 'anp_decoder.decoder.layers.4.fc2.weight', 'anp_decoder.decoder.layers.4.fc2.bias', 'anp_decoder.decoder.layers.4.final_layer_norm.weight', 'anp_decoder.decoder.layers.4.final_layer_norm.bias', 'anp_decoder.decoder.layers.5.self_attn.k_proj.weight', 'anp_decoder.decoder.layers.5.self_attn.k_proj.bias', 'anp_decoder.decoder.layers.5.self_attn.v_proj.weight', 'anp_decoder.decoder.layers.5.self_attn.v_proj.bias', 'anp_decoder.decoder.layers.5.self_attn.q_proj.weight', 'anp_decoder.decoder.layers.5.self_attn.q_proj.bias', 'anp_decoder.decoder.layers.5.self_attn.out_proj.weight', 'anp_decoder.decoder.layers.5.self_attn.out_proj.bias', 'anp_decoder.decoder.layers.5.self_attn_layer_norm.weight', 'anp_decoder.decoder.layers.5.self_attn_layer_norm.bias', 'anp_decoder.decoder.layers.5.encoder_attn.k_proj.weight', 'anp_decoder.decoder.layers.5.encoder_attn.k_proj.bias', 'anp_decoder.decoder.layers.5.encoder_attn.v_proj.weight', 'anp_decoder.decoder.layers.5.encoder_attn.v_proj.bias', 'anp_decoder.decoder.layers.5.encoder_attn.q_proj.weight', 'anp_decoder.decoder.layers.5.encoder_attn.q_proj.bias', 'anp_decoder.decoder.layers.5.encoder_attn.out_proj.weight', 'anp_decoder.decoder.layers.5.encoder_attn.out_proj.bias', 'anp_decoder.decoder.layers.5.encoder_attn_layer_norm.weight', 'anp_decoder.decoder.layers.5.encoder_attn_layer_norm.bias', 'anp_decoder.decoder.layers.5.fc1.weight', 'anp_decoder.decoder.layers.5.fc1.bias', 'anp_decoder.decoder.layers.5.fc2.weight', 'anp_decoder.decoder.layers.5.fc2.bias', 'anp_decoder.decoder.layers.5.final_layer_norm.weight', 'anp_decoder.decoder.layers.5.final_layer_norm.bias', 'anp_decoder.decoder.layernorm_embedding.weight', 'anp_decoder.decoder.layernorm_embedding.bias', 'anp_decoder.anp_head.dense.weight', 'anp_decoder.anp_head.dense.bias', 'anp_decoder.anp_head.out_proj.weight', 'anp_decoder.anp_head.out_proj.bias', 'encoder.layernorm_embedding.weight', 'encoder.layernorm_embedding.bias', 'mlm_decoder.decoder.layernorm_embedding.weight', 'mlm_decoder.decoder.layernorm_embedding.bias', 'mrm_decoder.decoder.layernorm_embedding.weight', 'mrm_decoder.decoder.layernorm_embedding.bias', 'span_decoder.decoder.layernorm_embedding.weight', 'span_decoder.decoder.layernorm_embedding.bias', 'anp_generate_decoder.decoder.layernorm_embedding.weight', 'anp_generate_decoder.decoder.layernorm_embedding.bias', 'senti_decoder.decoder.layernorm_embedding.weight', 'senti_decoder.decoder.layernorm_embedding.bias']
- This IS expected if you are initializing MultiModalBartModelForPretrain from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing MultiModalBartModelForPretrain from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2025-12-13 20:48:38,233 INFO All the weights of MultiModalBartModelForPretrain were initialized from the model checkpoint at ./checkpoint/pytorch_model.bin.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use MultiModalBartModelForPretrain for predictions without further training.
2025-12-13 20:48:38,555 DEBUG https://huggingface.co:443 "HEAD /facebook/bart-base/resolve/main/config.json HTTP/1.1" 307 0
2025-12-13 20:48:38,765 DEBUG https://huggingface.co:443 "HEAD /api/resolve-cache/models/facebook/bart-base/aadd2ab0ae0c8268c7c9693540e9904811f36177/config.json HTTP/1.1" 200 0
2025-12-13 20:48:39,106 DEBUG https://huggingface.co:443 "HEAD /facebook/bart-base/resolve/main/config.json HTTP/1.1" 307 0
2025-12-13 20:48:39,315 DEBUG https://huggingface.co:443 "HEAD /api/resolve-cache/models/facebook/bart-base/aadd2ab0ae0c8268c7c9693540e9904811f36177/config.json HTTP/1.1" 200 0
2025-12-13 20:48:41,637 DEBUG https://huggingface.co:443 "HEAD /facebook/bart-base/resolve/main/tokenizer_config.json HTTP/1.1" 404 0
2025-12-13 20:48:41,970 DEBUG https://huggingface.co:443 "GET /api/models/facebook/bart-base/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2025-12-13 20:48:42,288 DEBUG https://huggingface.co:443 "HEAD /facebook/bart-base/resolve/main/vocab.json HTTP/1.1" 307 0
2025-12-13 20:48:42,514 DEBUG https://huggingface.co:443 "HEAD /api/resolve-cache/models/facebook/bart-base/aadd2ab0ae0c8268c7c9693540e9904811f36177/vocab.json HTTP/1.1" 200 0
2025-12-13 20:48:44,001 INFO Loading data...
2025-12-13 20:48:44,114 INFO ==================== Epoch 1 =====================
2025-12-13 20:54:29,543 INFO DEV  aesc_p:54.39 aesc_r:17.66 aesc_f:26.67
2025-12-13 20:54:29,543 INFO TEST  aesc_p:55.77 aesc_r:20.92 aesc_f:30.43
2025-12-13 20:54:29,543 INFO DEV  ae_p:71.95 ae_r:23.37 ae_f:35.28  sc_p:25.2 sc_r:33.33 sc_f:28.7 sc_acc:75.59
2025-12-13 20:54:29,543 INFO TEST  ae_p:70.67 ae_r:26.51 ae_f:38.56  sc_p:26.3 sc_r:33.33 sc_f:29.4 sc_acc:78.91
2025-12-13 20:54:29,543 INFO Sentiment classification report (DEV):
2025-12-13 20:54:29,543 INFO class    precision   recall       f1  support
2025-12-13 20:54:29,543 INFO POS         75.59   100.00    86.10      192
2025-12-13 20:54:29,543 INFO NEU          0.00     0.00     0.00       20
2025-12-13 20:54:29,543 INFO NEG          0.00     0.00     0.00       42
2025-12-13 20:54:29,543 INFO Sentiment classification report (TEST):
2025-12-13 20:54:29,543 INFO class    precision   recall       f1  support
2025-12-13 20:54:29,543 INFO POS         78.91   100.00    88.21      232
2025-12-13 20:54:29,543 INFO NEU          0.00     0.00     0.00       22
2025-12-13 20:54:29,543 INFO NEG          0.00     0.00     0.00       40
2025-12-13 20:54:29,558 INFO save_pretrained failed: The weights trying to be saved contained shared tensors [{'decoder.decoder.embed_tokens.weight', 'encoder.embed_tokens.weight'}] that are mismatching the transformers base configuration. Try saving using `safe_serialization=False`, setting the `_dynamic_tied_weights_keys` attribute for affected modules, or remove this tensor sharing.. Trying fallback save...
2025-12-13 20:54:30,291 INFO ==================== Epoch 2 =====================
2025-12-13 20:57:27,337 INFO DEV  aesc_p:56.66 aesc_r:41.49 aesc_f:47.9
2025-12-13 20:57:27,337 INFO TEST  aesc_p:55.68 aesc_r:41.12 aesc_f:47.3
2025-12-13 20:57:27,337 INFO DEV  ae_p:71.48 ae_r:52.35 ae_f:60.44  sc_p:26.42 sc_r:33.33 sc_f:29.48 sc_acc:79.26
2025-12-13 20:57:27,337 INFO TEST  ae_p:70.7 ae_r:52.21 ae_f:60.06  sc_p:26.25 sc_r:33.33 sc_f:29.37 sc_acc:78.76
2025-12-13 20:57:27,337 INFO Sentiment classification report (DEV):
2025-12-13 20:57:27,337 INFO class    precision   recall       f1  support
2025-12-13 20:57:27,337 INFO POS         79.26   100.00    88.43      451
2025-12-13 20:57:27,337 INFO NEU          0.00     0.00     0.00       37
2025-12-13 20:57:27,337 INFO NEG          0.00     0.00     0.00       81
2025-12-13 20:57:27,337 INFO Sentiment classification report (TEST):
2025-12-13 20:57:27,337 INFO class    precision   recall       f1  support
2025-12-13 20:57:27,337 INFO POS         78.76   100.00    88.12      456
2025-12-13 20:57:27,337 INFO NEU          0.00     0.00     0.00       46
2025-12-13 20:57:27,337 INFO NEG          0.00     0.00     0.00       77
2025-12-13 20:57:27,358 INFO save_pretrained failed: The weights trying to be saved contained shared tensors [{'decoder.decoder.embed_tokens.weight', 'encoder.embed_tokens.weight'}] that are mismatching the transformers base configuration. Try saving using `safe_serialization=False`, setting the `_dynamic_tied_weights_keys` attribute for affected modules, or remove this tensor sharing.. Trying fallback save...
2025-12-13 20:57:28,054 INFO ==================== Epoch 3 =====================
2025-12-13 21:01:16,372 INFO DEV  aesc_p:60.56 aesc_r:59.89 aesc_f:60.22
2025-12-13 21:01:16,372 INFO TEST  aesc_p:56.12 aesc_r:57.08 aesc_f:56.59
2025-12-13 21:01:16,372 INFO DEV  ae_p:75.07 ae_r:74.24 ae_f:74.65  sc_p:42.17 sc_r:40.84 sc_f:41.49 sc_acc:80.67
2025-12-13 21:01:16,372 INFO TEST  ae_p:71.19 ae_r:72.41 ae_f:71.79  sc_p:43.16 sc_r:40.57 sc_f:41.83 sc_acc:78.83
2025-12-13 21:01:16,372 INFO Sentiment classification report (DEV):
2025-12-13 21:01:16,372 INFO class    precision   recall       f1  support
2025-12-13 21:01:16,388 INFO POS         84.08    95.85    89.58      650
2025-12-13 21:01:16,388 INFO NEU          0.00     0.00     0.00       52
2025-12-13 21:01:16,388 INFO NEG         42.42    26.67    32.75      105
2025-12-13 21:01:16,388 INFO Sentiment classification report (TEST):
2025-12-13 21:01:16,388 INFO class    precision   recall       f1  support
2025-12-13 21:01:16,388 INFO POS         81.21    97.58    88.64      620
2025-12-13 21:01:16,388 INFO NEU          0.00     0.00     0.00       67
2025-12-13 21:01:16,388 INFO NEG         48.28    24.14    32.18      116
2025-12-13 21:01:16,394 INFO save_pretrained failed: The weights trying to be saved contained shared tensors [{'decoder.decoder.embed_tokens.weight', 'encoder.embed_tokens.weight'}] that are mismatching the transformers base configuration. Try saving using `safe_serialization=False`, setting the `_dynamic_tied_weights_keys` attribute for affected modules, or remove this tensor sharing.. Trying fallback save...
2025-12-13 21:01:17,140 INFO ==================== Epoch 4 =====================
